{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problema 4.\n",
    "\n",
    "En primer lugar, github no permite subir archivos de mas de 25 mb, por lo tanto no se pueden subir los batches.\n",
    "Asi que le pido por favor que descarge el notebook y ponga los batches (supongo que los debe tener) en una carpeta llamada\n",
    "data y que este al lado del notebook, gracias.\n",

    "Para el desarrollo del modelo, que permita clasificar las imágenes de CIFAR10, se tomó en consideración\n",
    "cada uno de los parámetros estudiados en el problema 3. Antes de realizar a implementación, se estudiaron \n",
    "diversos modelos que permiten una buena clasificación de imagenes, pero la mayoría, por no decir todos, realizaban\n",
    "sus redes en base a redes convolusionales, lo cual parece ser o mejor en este caso.\n",
    "Como es necesario, por ahora, realizar redes feed forward, se utilizaron los parámetros, que mejor resultado \n",
    "mostraron en el problema anterior, es decir, cada parámetro junto al valor, que según lo observado, influía en una\n",
    "mejor y más rápida convergencia.\n",
    "Es por eso que, además de realizar un escalamiento de los datos, se utilizaron factores de Weight decay, un learning rate bajo, un factor\n",
    "de dropout en cada una de las capas ocultas donde cada una tenía una función de activación RELU, función que mostró\n",
    "mejores resultados en comparación a SIGMOID. \n",
    "Se utilizó además, un optimizador SGD, que nuevamente, en comparación a los otros, \n",
    "dió resultados de convergencia más rápido. (lr=0.001, decay=1e-6, momentum = 0.8) Este optimizador, utilizó un learning rate de 0.001,\n",
    "uno no tan chico, ni tan grande, un learning decay de 1e-6 y un momentum de 0,8. No se utilizo el momento clásico, puesto que no se\n",
    "demostró una gran mejoría.\n",
    "Se realizaron diversas pruebas utilizando para ello, un conjunto de validación de 10000 para training, y 3000 de testing. Después \n",
    "de algunas pruebas, se llega a la conclusión que, es necesario darle un gran número de capas ocultas, para que así se logre un mayor\n",
    "nivel de abstracción con las imágenes. Esto puede ser ya que de por si, las imagenes, al trabajar de pixel a pixel, tienen un nivel\n",
    "de no linealidad muy alta. Por lo que se le aumentó significativamente el número de estas, utilizando finalmente, 10 capas ocultas.\n",
    "Para la cantidad de epochs, se pensó que quizás un número alto, permetiría obtener un mejor resultado, lamentablemente no fue así. Pero \n",
    "finalmente, se le asignó un total de 700 epochs.\n",
    "\n",
    "Para el proceso de entrenamiento final, el modelo se implementó en un servidor, el cual gracias a la empresa donde uno de \n",
    "nosotros trabaja. Hacia este servidor, se puede acceder vía ssh, por lo que se configuró el ambiente, para que se puedan entrenar redes\n",
    "utilizando python y keras. Una vez listo, se realizó un pequeño script, para que la red se mantenga entrenando, aún después de \n",
    "que el usuario cerrara la sesión. \n",
    "El entrenamiento total, tomó alrededor de 4 días. Cerca del primer día, la red comenzó a tener overfitting, es decir, el valor de error\n",
    "en el entrenamiento comenzó a ser muy bajo, y el de accuracy en entrenamiento, casi cercano a 1 (100%), pero no así en el caso de los \n",
    "valores de testeo, estos comenzaron a estancarse en un 60% de exactitud.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import cPickle as pickle\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.misc import imread\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv1D,Conv2D, MaxPooling2D\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "import matplotlib\n",
    "# Force matplotlib to not use any Xwindows backend.\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "label_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog',\\\n",
    "'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "\n",
    "def load_CIFAR_one(filename):\n",
    "\twith open(filename, 'rb') as f:\n",
    "\t\tdatadict = pickle.load(f)\n",
    "\t\tX = datadict['data']\n",
    "\t\tY = datadict['labels']\n",
    "\t\tY = np.array(Y)\n",
    "\t\treturn X, Y\n",
    "\n",
    "def load_CIFAR10(PATH):\n",
    "\txs = []\n",
    "\tys = []\n",
    "\tfor b in range(1,6):\n",
    "\t\tf = os.path.join(PATH, 'data_batch_%d' % (b, ))\n",
    "\t\tX, Y = load_CIFAR_one(f)\n",
    "\t\txs.append(X)\n",
    "\t\tys.append(Y)\n",
    "\tXtr = np.concatenate(xs)\n",
    "\tYtr = np.concatenate(ys)\n",
    "\tdel X, Y\n",
    "\tXte, Yte = load_CIFAR_one(os.path.join(PATH, 'test_batch'))\n",
    "\treturn Xtr, Ytr, Xte, Yte\n",
    "\n",
    "Xtr,Ytr,Xte,Yte = load_CIFAR10('data/')\n",
    "\n",
    "Xva = Xtr[0:10000].copy()\n",
    "Yva = Ytr[0:10000].copy()\n",
    "\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "num_classes = 10\n",
    "epochs = 700\n",
    "\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "#Ytr = keras.utils.to_categorical(Ytr, num_classes)\n",
    "#Yte = keras.utils.to_categorical(Yte, num_classes)\n",
    "\n",
    "Xtr = Xtr.astype('float32')\n",
    "Xte= Xte.astype('float32')\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.regularizers import l1\n",
    "from keras.layers import Dropout\n",
    "\n",
    "scaler = StandardScaler().fit(Xtr)\n",
    "Xtrs = pd.DataFrame(scaler.transform(Xtr))\n",
    "#Xtes = pd.DataFrame(scaler.transform(Xte))\n",
    "\n",
    "scalerVa = StandardScaler().fit(Xva)\n",
    "Xvas = pd.DataFrame(scalerVa.transform(Xva))\n",
    "scalerVa = StandardScaler().fit(Xte)\n",
    "Xtes = pd.DataFrame(scaler.transform(Xte))\n",
    " \n",
    "# Convert class vectors to binary class matrices.\n",
    "Ytr = keras.utils.to_categorical(Ytr, num_classes)\n",
    "Yte = keras.utils.to_categorical(Yte, num_classes)\n",
    "Yva = keras.utils.to_categorical(Yva, num_classes)\n",
    "\n",
    "regu = 0.00001\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(2000, activation=\"relu\", kernel_initializer=\"uniform\", input_dim=Xtr.shape[1],W_regularizer=l1(regu)))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(1200, activation=\"relu\", kernel_initializer=\"uniform\",W_regularizer=l1(regu)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(1200, activation=\"relu\", kernel_initializer=\"uniform\",W_regularizer=l1(regu)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(1200, activation=\"relu\", kernel_initializer=\"uniform\",W_regularizer=l1(regu)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(1200, activation=\"relu\", kernel_initializer=\"uniform\",W_regularizer=l1(regu)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(1200, activation=\"relu\", kernel_initializer=\"uniform\",W_regularizer=l1(regu)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(1200, activation=\"relu\", kernel_initializer=\"uniform\",W_regularizer=l1(regu)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(1200, activation=\"relu\", kernel_initializer=\"uniform\",W_regularizer=l1(regu)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(1200, activation=\"relu\", kernel_initializer=\"uniform\",W_regularizer=l1(regu)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(512, activation=\"relu\", kernel_initializer=\"uniform\",W_regularizer=l1(regu)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(units=10, activation=\"softmax\"))\n",
    "\n",
    "\n",
    "sgd = SGD(lr=0.001, decay=1e-6, momentum = 0.8)\n",
    "model.compile(optimizer=sgd ,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "hist = model.fit(Xtrs.as_matrix(), Ytr, epochs=epochs,batch_size=batch_size, validation_data=(Xtes.as_matrix(), Yte))\n",
    "\n",
    "print(hist.history.keys())\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['loss train', 'loss test'], loc='upper left')\n",
    "plt.savefig('lossProblema4.jpg')\n",
    "\n",
    "plt.clf()\n",
    "plt.plot(hist.history['acc'])\n",
    "plt.plot(hist.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('acc')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['acc train','acc test'], loc='upper left')\n",
    "plt.savefig('accProblema4.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Lamentablemente no se logró entrenar la red, con la mejora de atributos expuestas en el punto d), por lo que no sé logró realizar\n",
    "una comparación entre ambos resultados. Aún así, se sub entiende, que al mejorar los parámetros que utiliza el modelo, se deberían \n",
    "lograr mejores resultados con la red.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
